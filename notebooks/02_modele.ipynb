{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Charge un fichier JSON et retourne son contenu.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Chemin du fichier JSON\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contenu du fichier JSON\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Chemins vers les fichiers JSON\n",
    "files = {\n",
    "    \"fr\": \"../data/questions_fr.json\",\n",
    "    \"en\": \"../data/questions_en.json\",\n",
    "    \"ar\": \"../data/questions_ar.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {lang: load_json(path) for lang, path in files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_answers(data, lang=\"fr\"):\n",
    "    \"\"\"\n",
    "    Extrait les questions et réponses d'un fichier de données JSON.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Données du chatbot\n",
    "        lang (str): Langue (fr/en/ar)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Liste des questions et des réponses\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for item in data[lang].get('questions', []):\n",
    "        questions.append(item['question'])\n",
    "        answers.append(item['answer'])\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_fr, answers_fr = extract_questions_answers(data, \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(questions, answers, max_len=20):\n",
    "    \"\"\"\n",
    "    Prétraite les données pour le modèle en tokenisant et en préparant les séquences.\n",
    "    \n",
    "    Args:\n",
    "        questions (list): Liste des questions\n",
    "        answers (list): Liste des réponses\n",
    "        max_len (int): Longueur maximale des séquences\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Séquences tokenisées et padées pour les questions et les réponses\n",
    "    \"\"\"\n",
    "    tokenizer_q = Tokenizer()\n",
    "    tokenizer_q.fit_on_texts(questions)\n",
    "    \n",
    "    # Initialisation du tokenizer pour les réponses\n",
    "    tokenizer_a = Tokenizer()\n",
    "    tokenizer_a.fit_on_texts(answers)\n",
    "    \n",
    "    # Ajout des tokens 'startseq' et 'endseq' dans le tokenizer des réponses\n",
    "    tokenizer_a.word_index['startseq'] = len(tokenizer_a.word_index) + 1\n",
    "    tokenizer_a.word_index['endseq'] = len(tokenizer_a.word_index) + 2\n",
    "    tokenizer_a.index_word[tokenizer_a.word_index['startseq']] = 'startseq'\n",
    "    tokenizer_a.index_word[tokenizer_a.word_index['endseq']] = 'endseq'\n",
    "\n",
    "    # Convertir les questions et réponses en séquences\n",
    "    seq_q = tokenizer_q.texts_to_sequences(questions)\n",
    "    seq_a = tokenizer_a.texts_to_sequences(answers)\n",
    "    \n",
    "    # Pad les séquences\n",
    "    seq_q_pad = pad_sequences(seq_q, maxlen=max_len, padding='post')\n",
    "    seq_a_pad = pad_sequences(seq_a, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Ajouter un token de début et de fin pour les réponses\n",
    "    seq_a_pad = np.array([np.append([tokenizer_a.word_index['startseq']], seq) for seq in seq_a_pad])\n",
    "    seq_a_pad = np.array([np.append(seq, [tokenizer_a.word_index['endseq']]) for seq in seq_a_pad])\n",
    "    \n",
    "    return seq_q_pad, seq_a_pad, tokenizer_q, tokenizer_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "seq_q_fr, seq_a_fr, tokenizer_q_fr, tokenizer_a_fr = preprocess_data(questions_fr, answers_fr, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq_model(input_dim, output_dim, max_len):\n",
    "    \"\"\"\n",
    "    Crée un modèle Seq2Seq simple avec LSTM.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Taille du vocabulaire d'entrée\n",
    "        output_dim (int): Taille du vocabulaire de sortie\n",
    "        max_len (int): Longueur maximale des séquences\n",
    "    \n",
    "    Returns:\n",
    "        Model: Modèle Keras\n",
    "    \"\"\"\n",
    "    # Entrée pour les questions\n",
    "    encoder_inputs = Input(shape=(max_len,))\n",
    "    encoder = LSTM(256, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    \n",
    "    # Sauvegarde des états pour la partie décodeur\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Entrée pour les réponses\n",
    "    decoder_inputs = Input(shape=(max_len,))\n",
    "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(output_dim, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Modèle final\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(tokenizer_q_fr.word_index) + 1\n",
    "output_dim = len(tokenizer_a_fr.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m build_seq2seq_model(input_dim, output_dim, max_len)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mbuild_seq2seq_model\u001b[1;34m(input_dim, output_dim, max_len)\u001b[0m\n\u001b[0;32m     14\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_len,))\n\u001b[0;32m     15\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LSTM(\u001b[38;5;241m256\u001b[39m, return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m encoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m encoder(encoder_inputs)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Sauvegarde des états pour la partie décodeur\u001b[39;00m\n\u001b[0;32m     19\u001b[0m encoder_states \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n",
      "File \u001b[1;32mc:\\Users\\Abdel\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Abdel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 20)"
     ]
    }
   ],
   "source": [
    "model = build_seq2seq_model(input_dim, output_dim, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
