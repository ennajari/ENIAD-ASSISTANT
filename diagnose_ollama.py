#!/usr/bin/env python3
"""
Diagnostic de l'√©tat d'Ollama
"""

import requests
import time
import psutil

def check_ollama_service():
    """V√©rifier le service Ollama"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return True, models
        return False, f"HTTP {response.status_code}"
    except Exception as e:
        return False, str(e)

def check_system_resources():
    """V√©rifier les ressources syst√®me"""
    try:
        # RAM
        memory = psutil.virtual_memory()
        ram_used = memory.percent
        ram_available = memory.available / (1024**3)  # GB
        
        # CPU
        cpu_percent = psutil.cpu_percent(interval=1)
        
        return {
            "ram_used_percent": ram_used,
            "ram_available_gb": ram_available,
            "cpu_percent": cpu_percent
        }
    except Exception as e:
        return {"error": str(e)}

def test_ollama_embedding():
    """Tester la g√©n√©ration d'embedding simple"""
    try:
        payload = {
            "model": "nomic-embed-text",
            "prompt": "test"
        }
        
        start_time = time.time()
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json=payload,
            timeout=60
        )
        end_time = time.time()
        
        if response.status_code == 200:
            duration = end_time - start_time
            result = response.json()
            embedding = result.get('embedding', [])
            return True, {
                "duration": duration,
                "embedding_size": len(embedding)
            }
        else:
            return False, f"HTTP {response.status_code}"
            
    except Exception as e:
        return False, str(e)

def main():
    """Diagnostic principal"""
    print("üîç DIAGNOSTIC OLLAMA")
    print("=" * 30)
    
    # 1. Service Ollama
    print("1. Service Ollama...")
    ollama_ok, ollama_info = check_ollama_service()
    if ollama_ok:
        print(f"   ‚úÖ Ollama accessible")
        print(f"   üì¶ Mod√®les: {len(ollama_info)}")
        for model in ollama_info[:3]:  # Afficher les 3 premiers
            name = model.get('name', 'Unknown')
            size = model.get('size', 0) / (1024**3)  # GB
            print(f"      - {name} ({size:.1f}GB)")
    else:
        print(f"   ‚ùå Ollama: {ollama_info}")
        print("   üí° D√©marrez Ollama: ollama serve")
        return
    
    # 2. Ressources syst√®me
    print("\n2. Ressources syst√®me...")
    resources = check_system_resources()
    if "error" not in resources:
        print(f"   üíæ RAM utilis√©e: {resources['ram_used_percent']:.1f}%")
        print(f"   üíæ RAM disponible: {resources['ram_available_gb']:.1f}GB")
        print(f"   üñ•Ô∏è CPU: {resources['cpu_percent']:.1f}%")
        
        # Alertes
        if resources['ram_used_percent'] > 80:
            print("   ‚ö†Ô∏è RAM critique (>80%)")
        if resources['ram_available_gb'] < 2:
            print("   ‚ö†Ô∏è RAM disponible faible (<2GB)")
        if resources['cpu_percent'] > 80:
            print("   ‚ö†Ô∏è CPU surcharg√© (>80%)")
    else:
        print(f"   ‚ùå Erreur: {resources['error']}")
    
    # 3. Test d'embedding
    print("\n3. Test d'embedding...")
    print("   ‚è≥ G√©n√©ration d'un embedding de test...")
    
    embed_ok, embed_info = test_ollama_embedding()
    if embed_ok:
        duration = embed_info['duration']
        size = embed_info['embedding_size']
        print(f"   ‚úÖ Embedding g√©n√©r√© en {duration:.2f}s")
        print(f"   üìä Taille: {size} dimensions")
        
        # Analyse de performance
        if duration < 5:
            print("   üöÄ Performance excellente")
        elif duration < 15:
            print("   ‚úÖ Performance acceptable")
        elif duration < 30:
            print("   ‚ö†Ô∏è Performance lente")
        else:
            print("   ‚ùå Performance tr√®s lente")
            print("   üí° Consid√©rez red√©marrer Ollama")
    else:
        print(f"   ‚ùå √âchec: {embed_info}")
    
    # 4. Recommandations
    print("\nüí° RECOMMANDATIONS:")
    
    if not ollama_ok:
        print("- D√©marrez Ollama: ollama serve")
    elif not embed_ok:
        print("- Red√©marrez Ollama")
        print("- V√©rifiez les mod√®les install√©s")
    elif embed_ok and embed_info.get('duration', 0) > 30:
        print("- Ollama est tr√®s lent, red√©marrage recommand√©")
        print("- Fermez d'autres applications pour lib√©rer la RAM")
    else:
        print("- Ollama fonctionne correctement")
        print("- Le probl√®me vient probablement du volume de donn√©es √† indexer")
        print("- Utilisez fix_indexation_ultra_light.py pour indexer progressivement")

if __name__ == "__main__":
    main()
