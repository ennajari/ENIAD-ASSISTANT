#!/usr/bin/env python3
"""
Test script to verify model selector functionality
"""

import requests
import json

def test_gemini_model():
    """Test Gemini model via SMA service"""
    print("üß™ Testing Gemini Model...")
    
    try:
        # Mock SMA results for testing
        mock_sma_results = {
            "results": [
                {
                    "title": "Test ENIAD Information",
                    "content": "Information de test pour d√©montrer le fonctionnement du mod√®le Gemini avec contexte SMA",
                    "source_url": "https://eniad.ump.ma/fr"
                }
            ],
            "sources": [{"url": "https://eniad.ump.ma/fr", "title": "ENIAD"}],
            "metadata": {"confidence": 0.9},
            "total_found": 1
        }
        
        payload = {
            "query": "Test du mod√®le Gemini avec s√©lecteur",
            "language": "fr",
            "sma_results": mock_sma_results
        }
        
        response = requests.post(
            "http://localhost:8001/sma/chat-with-context",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            print("‚úÖ Gemini model working!")
            print(f"   Model: {data.get('model', 'unknown')}")
            print(f"   Provider: {data.get('provider', 'unknown')}")
            print(f"   Answer preview: {data.get('final_answer', '')[:100]}...")
            return True
        else:
            print(f"‚ùå Gemini model failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Gemini model error: {e}")
        return False

def test_llama_model():
    """Test Llama model (if available)"""
    print("\nü¶ô Testing Llama Model...")
    
    try:
        # Test if RAG service is available for Llama model
        response = requests.get("http://localhost:8000/health", timeout=5)
        
        if response.status_code == 200:
            print("‚úÖ Llama model service available!")
            print("   RAG service is running on port 8000")
            return True
        else:
            print("‚ö†Ô∏è Llama model service not available")
            print("   RAG service not running on port 8000")
            return False
            
    except Exception as e:
        print("‚ö†Ô∏è Llama model service not available")
        print(f"   Error: {e}")
        return False

def test_sma_intelligent_query():
    """Test SMA intelligent query endpoint"""
    print("\nüß† Testing SMA Intelligent Query...")
    
    try:
        payload = {
            "query": "Test de s√©lection de mod√®le avec SMA",
            "language": "fr",
            "search_depth": "medium",
            "include_documents": True,
            "include_images": True,
            "max_results": 3
        }
        
        response = requests.post(
            "http://localhost:8001/sma/intelligent-query",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            print("‚úÖ SMA intelligent query working!")
            print(f"   Query: {data.get('query', 'unknown')}")
            print(f"   Language: {data.get('language', 'unknown')}")
            print(f"   Items found: {data.get('total_items_found', 0)}")
            return True
        else:
            print(f"‚ùå SMA intelligent query failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå SMA intelligent query error: {e}")
        return False

def main():
    print("üéì MODEL SELECTOR FUNCTIONALITY TEST")
    print("=" * 50)
    
    # Test Gemini model
    gemini_ok = test_gemini_model()
    
    # Test Llama model
    llama_ok = test_llama_model()
    
    # Test SMA intelligent query
    sma_ok = test_sma_intelligent_query()
    
    print("\n" + "=" * 50)
    print("üìä TEST RESULTS:")
    print(f"‚úÖ Gemini Model: {'Working' if gemini_ok else 'Failed'}")
    print(f"‚úÖ Llama Model: {'Available' if llama_ok else 'Not Available'}")
    print(f"‚úÖ SMA Service: {'Working' if sma_ok else 'Failed'}")
    
    print("\nüéØ Model Selector Status:")
    if gemini_ok:
        print("‚Ä¢ ‚ú® Gemini API: Ready for intelligent responses")
    if llama_ok:
        print("‚Ä¢ ü¶ô Llama Model: Ready for custom ENIAD responses")
    else:
        print("‚Ä¢ ‚ö†Ô∏è Llama Model: Not available (RAG service not running)")
    
    if sma_ok:
        print("‚Ä¢ üß† SMA System: Ready for web intelligence")
    
    print("\nüìã Interface Instructions:")
    print("1. Open http://localhost:5174")
    print("2. Look for the Model Selector above the input")
    print("3. Choose between:")
    print("   ‚Ä¢ Gemini API (Google's advanced AI)")
    print("   ‚Ä¢ Llama (Your custom ENIAD project)")
    print("4. Click SMA button for web scraping")
    print("5. Ask questions and compare results!")
    
    print("\nüí° What you'll see:")
    print("‚Ä¢ Different response styles between models")
    print("‚Ä¢ Gemini: More general, well-structured responses")
    print("‚Ä¢ Llama: More ENIAD-specific, customized responses")
    print("‚Ä¢ SMA: Enhanced with real-time web content")

if __name__ == "__main__":
    main()
