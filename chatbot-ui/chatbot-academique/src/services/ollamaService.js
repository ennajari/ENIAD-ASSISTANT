/**
 * Service Ollama pour modÃ¨les locaux
 * IntÃ©gration avec Llama 3.2 pour les tests RAG
 */

import axios from 'axios';

class OllamaService {
  constructor() {
    this.baseUrl = 'http://localhost:11434';
    this.isAvailable = false;
    this.currentModel = 'llama3.2:1b'; // ModÃ¨le lÃ©ger parfait pour 8GB RAM
    this.ollamaPath = 'C:\\Users\\ROG FLOW\\AppData\\Local\\Programs\\Ollama\\ollama.exe';
    this.checkAvailability();

    console.log('ğŸ¦™ Ollama Service initialized with model:', this.currentModel);
  }

  /**
   * VÃ©rifier la disponibilitÃ© d'Ollama
   */
  async checkAvailability() {
    try {
      const response = await axios.get(`${this.baseUrl}/api/tags`, { timeout: 5000 });
      this.isAvailable = response.status === 200;
      console.log('âœ… Ollama is available');
      
      // VÃ©rifier les modÃ¨les installÃ©s
      const models = response.data.models || [];
      console.log('ğŸ“š ModÃ¨les disponibles:', models.map(m => m.name));
      
      return true;
    } catch (error) {
      this.isAvailable = false;
      console.log('âš ï¸ Ollama not available:', error.message);
      return false;
    }
  }

  /**
   * Installer un modÃ¨le Llama
   */
  async installModel(modelName = 'llama3.2:1b') {
    try {
      console.log(`ğŸ“¥ Installation du modÃ¨le ${modelName}...`);
      
      const response = await axios.post(`${this.baseUrl}/api/pull`, {
        name: modelName
      }, { 
        timeout: 300000, // 5 minutes pour l'installation
        responseType: 'stream'
      });

      return {
        success: true,
        message: `ModÃ¨le ${modelName} installÃ© avec succÃ¨s`
      };
    } catch (error) {
      console.error('âŒ Erreur installation modÃ¨le:', error);
      return {
        success: false,
        message: `Erreur installation: ${error.message}`
      };
    }
  }

  /**
   * GÃ©nÃ©rer une rÃ©ponse avec Ollama
   */
  async generateResponse(prompt, options = {}) {
    if (!this.isAvailable) {
      throw new Error('Ollama n\'est pas disponible');
    }

    try {
      const {
        model = this.currentModel,
        temperature = 0.3,
        maxTokens = 500,
        stream = false
      } = options;

      console.log(`ğŸ¤– GÃ©nÃ©ration avec ${model}:`, prompt.substring(0, 100) + '...');

      const response = await axios.post(`${this.baseUrl}/api/generate`, {
        model: model,
        prompt: prompt,
        stream: stream,
        options: {
          temperature: temperature,
          num_predict: maxTokens,
          top_p: 0.9,
          top_k: 40
        }
      }, { timeout: 30000 });

      if (response.data && response.data.response) {
        return {
          success: true,
          response: response.data.response,
          model: model,
          done: response.data.done || true
        };
      } else {
        throw new Error('RÃ©ponse invalide d\'Ollama');
      }
    } catch (error) {
      console.error('âŒ Erreur gÃ©nÃ©ration Ollama:', error);
      throw error;
    }
  }

  /**
   * GÃ©nÃ©rer une rÃ©ponse de chat
   */
  async generateChatResponse(messages, options = {}) {
    try {
      // Convertir les messages en prompt simple
      const prompt = messages.map(msg => {
        if (msg.role === 'system') {
          return `Instructions: ${msg.content}`;
        } else if (msg.role === 'user') {
          return `Utilisateur: ${msg.content}`;
        } else if (msg.role === 'assistant') {
          return `Assistant: ${msg.content}`;
        }
        return msg.content;
      }).join('\n\n') + '\n\nAssistant:';

      const result = await this.generateResponse(prompt, options);
      
      return {
        choices: [{
          message: {
            role: 'assistant',
            content: result.response
          }
        }],
        model: result.model,
        usage: {
          total_tokens: result.response.length // Approximation
        }
      };
    } catch (error) {
      console.error('âŒ Erreur chat Ollama:', error);
      throw error;
    }
  }

  /**
   * GÃ©nÃ©rer des embeddings (simulation simple)
   */
  async generateEmbeddings(text, options = {}) {
    try {
      // Pour l'instant, simulation simple
      // Ollama ne supporte pas encore les embeddings directement
      console.log('âš ï¸ Embeddings simulÃ©s pour Ollama');
      
      // GÃ©nÃ©rer un vecteur simple basÃ© sur le texte
      const words = text.toLowerCase().split(/\s+/);
      const embedding = new Array(384).fill(0); // Dimension standard
      
      // Hash simple pour crÃ©er un vecteur reproductible
      for (let i = 0; i < words.length; i++) {
        const word = words[i];
        for (let j = 0; j < word.length; j++) {
          const charCode = word.charCodeAt(j);
          embedding[charCode % 384] += 1;
        }
      }
      
      // Normaliser
      const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
      const normalizedEmbedding = embedding.map(val => val / (norm || 1));
      
      return {
        success: true,
        embedding: normalizedEmbedding,
        model: 'ollama-simulation'
      };
    } catch (error) {
      console.error('âŒ Erreur embeddings Ollama:', error);
      throw error;
    }
  }

  /**
   * Lister les modÃ¨les disponibles
   */
  async listModels() {
    try {
      const response = await axios.get(`${this.baseUrl}/api/tags`);
      return {
        success: true,
        models: response.data.models || []
      };
    } catch (error) {
      console.error('âŒ Erreur liste modÃ¨les:', error);
      return {
        success: false,
        models: []
      };
    }
  }

  /**
   * Changer le modÃ¨le actuel
   */
  setModel(modelName) {
    this.currentModel = modelName;
    console.log(`ğŸ”„ ModÃ¨le changÃ© vers: ${modelName}`);
  }

  /**
   * Obtenir le statut du service
   */
  async getStatus() {
    const available = await this.checkAvailability();
    const models = await this.listModels();
    
    return {
      service: 'Ollama Local',
      available: available,
      currentModel: this.currentModel,
      baseUrl: this.baseUrl,
      models: models.models,
      lastCheck: new Date().toISOString()
    };
  }

  /**
   * RÃ©ponse optimisÃ©e pour ENIAD
   */
  async generateENIADResponse(query, language = 'fr', context = '') {
    try {
      const systemPrompt = language === 'ar' ? 
        'Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù„Ù…Ø¹Ù‡Ø¯ ENIAD Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ¹Ù„ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø£Ø¬Ø¨ Ø¨Ø¯Ù‚Ø© ÙˆÙˆØ¶ÙˆØ­.' :
        'Tu es l\'assistant acadÃ©mique ENIAD spÃ©cialisÃ© en intelligence artificielle et science des donnÃ©es. RÃ©ponds avec prÃ©cision et clartÃ©.';

      const contextPrompt = context ? 
        (language === 'ar' ? `Ø§Ù„Ø³ÙŠØ§Ù‚: ${context}\n\n` : `Contexte: ${context}\n\n`) : '';

      const fullPrompt = `${systemPrompt}\n\n${contextPrompt}Question: ${query}\n\nRÃ©ponse:`;

      const result = await this.generateResponse(fullPrompt, {
        temperature: 0.3,
        maxTokens: 400
      });

      return {
        success: true,
        answer: result.response,
        model: result.model,
        source: 'Ollama Local',
        icon: 'ğŸ¦™'
      };
    } catch (error) {
      console.error('âŒ Erreur rÃ©ponse ENIAD:', error);
      return {
        success: false,
        answer: language === 'ar' ? 
          'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ.' : 
          'DÃ©solÃ©, erreur avec le modÃ¨le local.',
        model: 'error'
      };
    }
  }
}

// Export singleton
const ollamaService = new OllamaService();
export default ollamaService;
